{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valeur moyenne quand il y a incendie \n",
      " Temperature[C]       14.901369\n",
      "Humidity[%]          51.367693\n",
      "TVOC[ppb]           810.682730\n",
      "eCO2[ppm]           405.007016\n",
      "Raw H2            13025.023771\n",
      "Raw Ethanol       19675.249566\n",
      "Pressure[hPa]       939.071326\n",
      "PM1.0                 1.593498\n",
      "PM2.5                 1.655554\n",
      "NC0.5                10.967304\n",
      "NC1.0                 1.710226\n",
      "NC2.5                 0.038628\n",
      "dtype: float64 \n",
      "\n",
      "valeur moyenne quand il y a pas incendie \n",
      " Temperature[C]       18.200738\n",
      "Humidity[%]          48.546970\n",
      "TVOC[ppb]            76.003601\n",
      "eCO2[ppm]           405.579496\n",
      "Raw H2            12945.476028\n",
      "Raw Ethanol       20269.400985\n",
      "Pressure[hPa]       938.676613\n",
      "PM1.0                 1.193409\n",
      "PM2.5                 1.241376\n",
      "NC0.5                 8.210595\n",
      "NC1.0                 1.282602\n",
      "NC2.5                 0.030301\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "\n",
    "param_grids = {\n",
    "    'Random Forest': {'n_estimators': [100, 200], 'max_depth': [5, 10]},\n",
    "    'Neural Network': {'hidden_layer_sizes': [(100,), (50, 50)], 'activation': ['relu', 'tanh']},\n",
    "    'XGBoost': {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01], 'n_estimators': [100, 200]},\n",
    "}\n",
    "#Xgboost\n",
    "#random forest\n",
    "#Reseau de neurone\n",
    "#Linear regression\n",
    "\n",
    "# Validation croisée avec plusieurs métriques\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score)\n",
    "}\n",
    "\n",
    "numerical_col = ['Temperature[C]','Humidity[%]','TVOC[ppb]','eCO2[ppm]','Raw H2','Raw Ethanol','Pressure[hPa]','PM1.0','PM2.5','NC0.5','NC1.0','NC2.5']\n",
    "data = pd.read_csv('../data/processed/clean_dataset_.csv')\n",
    "data_inc = data[data['Fire Alarm']==1]\n",
    "data_no_inc = data[data['Fire Alarm']== 0]\n",
    "mean_values_inc = data_inc[numerical_col].mean()\n",
    "mean_values_no_inc = data_no_inc[numerical_col].mean()\n",
    "print(\"valeur moyenne quand il y a incendie \\n\", mean_values_inc, \"\\n\")\n",
    "print(\"valeur moyenne quand il y a pas incendie \\n\", mean_values_no_inc)\n",
    "\n",
    "components = pd.read_csv('../data/processed/principalComponents_.csv')\n",
    "\n",
    "X = components #composantes principales obtenu après l'ACP\n",
    "y = data['Fire Alarm'] #variable cible\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#fonction qui permet d'évaluer chaque modèle\n",
    "def evaluer_modele(y_test, y_pred, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule et affiche les principales métriques d'évaluation d'un modèle de classification.\n",
    "    Args:\n",
    "        y_test (array-like): Les vraies étiquettes.\n",
    "        y_pred (array-like): Les étiquettes prédites par le modèle.\n",
    "        y_pred_proba (array-like): Les probabilités prédites pour la classe positive.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Création du DataFrame pour afficher les résultats\n",
    "    data = {'Métrique': ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC', 'Matrice de confusion'],\n",
    "            'Valeur': [accuracy, precision, recall, f1, roc_auc, conf_matrix]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meilleur paramètres:  {'max_depth': None, 'n_estimators': 100}\n",
      "meilleur score:  0.9694358220883116\n",
      "----------------------------------Score moyen d'entrainement---------------------------------------\n",
      "Accuracy moyenne: 0.9698739964398483\n",
      "Precision moyenne: 0.9763835407588971\n",
      "Recall moyenne: 0.9828560413404889\n",
      "F1-score moyen: 0.979607206692552\n",
      "AUC moyenne: 0.958255153717866\n",
      "----------------------------------Score moyen après prédictions---------------------------------------\n",
      "Accuracy moyenne: 0.9630510882964609\n",
      "Precision moyenne: 0.9712295473891164\n",
      "Recall moyenne: 0.978654648347481\n",
      "F1-score moyen: 0.9749151763403414\n",
      "AUC moyenne: 0.9493835736201042\n",
      "\n",
      " [[1992  136]\n",
      " [ 108 5748]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/randomForest.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={\n",
    "    'n_estimators': [10, 30, 50, 100],\n",
    "    'max_depth': [None, 10, 20]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"meilleur paramètres: \", grid.best_params_)\n",
    "print(f\"meilleur score: \", grid.best_score_)\n",
    "\n",
    "#modèle le plus performants\n",
    "\n",
    "# Créer un modèle de forêt aléatoire\n",
    "model1 = grid.best_estimator_\n",
    "# Entraîner le modèle sur toutes les données (après avoir ajusté les hyperparamètres si nécessaire)\n",
    "model1.fit(X_train, y_train)\n",
    "#validation croisée avec multiscoring\n",
    "scores = cross_validate(model1, X_train, y_train, cv=10, scoring=scoring)\n",
    "# Afficher les scores moyens\n",
    "print(\"----------------------------------Score moyen d'entrainement---------------------------------------\")\n",
    "print(\"Accuracy moyenne:\", scores['test_accuracy'].mean())\n",
    "print(\"Precision moyenne:\", scores['test_precision'].mean())\n",
    "print(\"Recall moyenne:\", scores['test_recall'].mean())\n",
    "print(\"F1-score moyen:\", scores['test_f1'].mean())\n",
    "print(\"AUC moyenne:\", scores['test_roc_auc'].mean())\n",
    "\n",
    "y_pred1 = model1.predict(X_test)\n",
    "\n",
    "scores = cross_validate(model1, X_test, y_test, cv=10, scoring=scoring)\n",
    "# Afficher les scores moyens\n",
    "print(\"----------------------------------Score moyen après prédictions---------------------------------------\")\n",
    "print(\"Accuracy moyenne:\", scores['test_accuracy'].mean())\n",
    "print(\"Precision moyenne:\", scores['test_precision'].mean())\n",
    "print(\"Recall moyenne:\", scores['test_recall'].mean())\n",
    "print(\"F1-score moyen:\", scores['test_f1'].mean())\n",
    "print(\"AUC moyenne:\", scores['test_roc_auc'].mean())\n",
    "mat_conf = confusion_matrix(y_test, y_pred1)\n",
    "print(\"\\n\", mat_conf)\n",
    "joblib.dump(model1, '../models/randomForest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réseau de neurone\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meilleur paramètres:  {'max_depth': None, 'n_estimators': 100}\n",
      "meilleur score:  0.9694358220883116\n",
      "------------- Scores moyen avant d'entraînement -------------- \n",
      "\n",
      "Accuracy moyenne: 0.9678698373731989\n",
      "Precision moyenne: 0.9741961096439139\n",
      "Recall moyenne: 0.9823880467343005\n",
      "F1-score moyen: 0.9782717693602182\n",
      "AUC moyenne:  0.9548766352357454 \n",
      "\n",
      "--------------- Score moyen après prédictions -----------------\n",
      "Accuracy moyenne: 0.959671236915819\n",
      "Precision moyenne: 0.9694951996227061\n",
      "Recall moyenne: 0.9757533327499198\n",
      "F1-score moyen: 0.9725986985214721\n",
      "AUC moyenne: 0.9455810688906834\n",
      "\n",
      " [[1982  146]\n",
      " [  90 5766]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/xgboost.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7], \n",
    "    'learning_rate': [0.1, 0.01], \n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "\n",
    "grid2 = GridSearchCV(XGBClassifier(), param_grid, cv=10)\n",
    "grid2.fit(X_train, y_train)\n",
    "\n",
    "print(f\"meilleur paramètres: \", grid.best_params_)\n",
    "print(f\"meilleur score: \", grid.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "# Créer un modèle XGBoost\n",
    "model2 = grid2.best_estimator_\n",
    "scores2 = cross_validate(model2,X_train, y_train, cv=10, scoring=scoring)\n",
    "\n",
    "\n",
    "# Entraîner le modèle\n",
    "model2.fit(X_train, y_train)\n",
    "# Afficher les scores moyens d'entrainement\n",
    "print(\"------------- Scores moyen d'entraînement -------------- \\n\")\n",
    "print(\"Accuracy moyenne:\", scores2['test_accuracy'].mean())\n",
    "print(\"Precision moyenne:\", scores2['test_precision'].mean())\n",
    "print(\"Recall moyenne:\", scores2['test_recall'].mean())\n",
    "print(\"F1-score moyen:\", scores2['test_f1'].mean())\n",
    "print(\"AUC moyenne: \", scores2['test_roc_auc'].mean(),\"\\n\")\n",
    "# Faire des prédictions\n",
    "y_pred2 = model2.predict(X_test)\n",
    "score2_ = cross_validate(model2, X_test, y_test, cv=10, scoring=scoring)\n",
    "\n",
    "#afficher les scores moyens après prédictions\n",
    "print(\"--------------- Score moyen après prédictions -----------------\")\n",
    "print(\"Accuracy moyenne:\", score2_['test_accuracy'].mean())\n",
    "print(\"Precision moyenne:\", score2_['test_precision'].mean())\n",
    "print(\"Recall moyenne:\", score2_['test_recall'].mean())\n",
    "print(\"F1-score moyen:\", score2_['test_f1'].mean())\n",
    "print(\"AUC moyenne:\", score2_['test_roc_auc'].mean())\n",
    "\n",
    "#Matrice de confusion du modèle de xgboost\n",
    "matr_conf = confusion_matrix(y_test, y_pred2)\n",
    "print(\"\\n\", matr_conf)\n",
    "joblib.dump(model2, '../models/xgboost.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur quadratique moyenne (MSE): 0.08111904845057022\n",
      "Coefficient de détermination (R²): 0.5822101752033528\n",
      "Mean Squared Error: 0.08033216155873989\n",
      "R-squared: 0.5890794905236905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/regressionLineaire.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Créer un modèle de régression linéaire\n",
    "model3 = LinearRegression()\n",
    "\n",
    "# Validation croisée\n",
    "scores3 = cross_validate(model3, X_train, y_train, cv=10, scoring={'mse': 'neg_mean_squared_error', 'r2': 'r2'})\n",
    "\n",
    "print('Erreur quadratique moyenne (MSE):', -scores3['test_mse'].mean())\n",
    "print('Coefficient de détermination (R²):', scores3['test_r2'].mean())\n",
    "\n",
    "# Entraîner le modèle\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred3 = model3.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "mse = mean_squared_error(y_test, y_pred3)\n",
    "r2 = r2_score(y_test, y_pred3)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "joblib.dump(model3, '../models/regressionLineaire.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
